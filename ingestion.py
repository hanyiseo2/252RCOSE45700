import os
from dotenv import load_dotenv
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

load_dotenv()

if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")

print("ğŸ“¥ ë°ì´í„° ë¡œë”© ì‹œì‘...")

local_pdfs = [
    "./docs/wellarchitected-machine-learning-lens.pdf",
    "./docs/generative-ai-lens.pdf",
    "./docs/responsible-ai-lens.pdf",
]

urls = [
    # === Lenses ===
    "https://docs.aws.amazon.com/wellarchitected/latest/responsible-ai-lens/responsible-ai-lens.html",
    "https://docs.aws.amazon.com/wellarchitected/latest/generative-ai-lens/generative-ai-lens.html",
    "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/machine-learning-lens.html",
    "https://docs.aws.amazon.com/wellarchitected/latest/modern-industrial-data-technology-lens/modern-industrial-data-technology-lens.html",
    "https://docs.aws.amazon.com/wellarchitected/latest/end-user-computing-lens/end-user-computing-lens.html",
    "https://docs.aws.amazon.com/wellarchitected/latest/supply-chain-lens/supply-chain-lens.html",
    "https://docs.aws.amazon.com/wellarchitected/latest/iot-lens/iot-lens.html",
    "https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/high-performance-computing-lens.html?did=wp_card&trk=wp_card",
    "https://docs.aws.amazon.com/wellarchitected/latest/mergers-and-acquisitions-lens/mergers-and-acquisitions-lens.html?did=wp_card&trk=wp_card",
    "https://docs.aws.amazon.com/wellarchitected/latest/migration-lens/migration-lens.html?did=wp_card&trk=wp_card",
    "https://docs.aws.amazon.com/wellarchitected/latest/government-lens/government-lens.html?did=wp_card&trk=wp_card",
    "https://docs.aws.amazon.com/wellarchitected/latest/connected-mobility-lens/connected-mobility-lens.html?did=wp_card&trk=wp_card",
    "https://docs.aws.amazon.com/wellarchitected/latest/analytics-lens/analytics-lens.html?did=wp_card&trk=wp_card",
    ]

all_docs = []

if local_pdfs:
    print("ğŸ“„ ë¡œì»¬ PDF ë¡œë”© ì¤‘...")
    for pdf_path in local_pdfs:
        if os.path.exists(pdf_path):
            try:
                loader = PyPDFLoader(pdf_path)
                docs = loader.load()
                all_docs.extend(docs)
                print(f"  âœ… {pdf_path}: {len(docs)}í˜ì´ì§€")
            except Exception as e:
                print(f"  âŒ {pdf_path}: {e}")
        else:
            print(f"  âš ï¸  {pdf_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# ì›¹í˜ì´ì§€ ë¡œë“œ
successful_docs = []
failed_urls = []

print(f"ğŸ“„ ì´ {len(urls)}ê°œ URL ë¡œë”© ì‹œì‘...\n")

for idx, url in enumerate(urls, 1):
    try:
        url_name = url.split('/')[-1][:50]
        print(f"  [{idx}/{len(urls)}] {url_name}...", end=" ")
        loader = WebBaseLoader([url])
        docs = loader.load()
        all_docs.extend(docs)
        print("âœ…")
    except Exception as e:
        print(f"âŒ {str(e)[:30]}")
        failed_urls.append(url)

print(f"\nâœ… {len(all_docs)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
if failed_urls:
    print(f"âš ï¸  {len(failed_urls)}ê°œ URL ë¡œë“œ ì‹¤íŒ¨:")
    for url in failed_urls:
        print(f"   - {url.split('/')[-1]}")

# í…ìŠ¤íŠ¸ ì²­í‚¹
print("\nâœ‚ï¸ ë¬¸ì„œ ì²­í‚¹ ì¤‘...")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=150,
    separators=["\n\n", "\n", ". ", " ", ""]
)
chunks = text_splitter.split_documents(all_docs)
print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")

# ì²­í¬ ë¶„í¬ í™•ì¸
print("\nğŸ“‹ ì²­í¬ ë¶„í¬ (ìƒìœ„ 10ê°œ):")
source_count = {}
for chunk in chunks:
    source = chunk.metadata.get('source', 'Unknown')
    source_name = source.split('/')[-1].replace('.html', '').replace('-', ' ').title()
    source_count[source_name] = source_count.get(source_name, 0) + 1

for idx, (name, count) in enumerate(sorted(source_count.items(), key=lambda x: x[1], reverse=True)[:10], 1):
    print(f"  {idx}. {name}: {count}ê°œ")

if len(source_count) > 10:
    print(f"  ... ì™¸ {len(source_count) - 10}ê°œ")

# ì„ë² ë”© ìƒì„±
print("\nğŸ”¢ ì„ë² ë”© ìƒì„± ì¤‘... (ì•½ 30ì´ˆ~1ë¶„ ì†Œìš”)")
embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small"
    )
vectorstore = FAISS.from_documents(chunks, embeddings)
print("âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")

# ì €ì¥
print("\nğŸ’¾ ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì¤‘...")
vectorstore.save_local("vectorstore")
print("âœ… ì €ì¥ ì™„ë£Œ: ./vectorstore/")

# í†µê³„
avg_chunk_size = sum(len(c.page_content) for c in chunks) // len(chunks)
print("\n" + "="*60)
print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
print(f"  ğŸ“„ ë¬¸ì„œ: {len(all_docs)}ê°œ")
print(f"  âœ‚ï¸  ì²­í¬: {len(chunks)}ê°œ")
print(f"  ğŸ“ í‰ê·  ì²­í¬ í¬ê¸°: {avg_chunk_size}ì")
print(f"  ğŸ“‚ ì €ì¥ ìœ„ì¹˜: ./vectorstore/")
print("="*60)